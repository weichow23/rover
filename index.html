<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>ROVER</title>
    <link rel="icon" type="image/x-icon" href="./static/img/icons/brain.ico">
    
    <!-- Meta Tags -->
    <meta property="og:url" content="https://vision-x-nyu.github.io/thinking-in-space.github.io/" />
    <meta property="og:image" content="./static/img/preview.png" />
    <meta property="og:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
    
    <meta name="twitter:url" content="https://vision-x-nyu.github.io/thinking-in-space.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="./static/img/preview.png" />
    <meta name="twitter:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
    <meta name="twitter:description" content="We introduce VSI-Bench, a novel benchmark of over 5,000 video-based visual-spatial intelligence questions, to evaluate and probe MLLMs." />

    <!-- Preload critical resources -->
    <link rel="preload" href="./static/css/style.css" as="style">
    <link rel="preload" href="./static/img/preview.png" as="image">
    <link rel="preconnect" href="https://polyfill.io">
    <link rel="preconnect" href="https://cdn.jsdelivr.net">
    <link rel="preconnect" href="https://d3js.org">

    <!-- Critical CSS inlined -->
    <style>
        /* Critical path CSS - Basic layout and above-the-fold content */
        body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; }
        .header-wrapper { background: #fff; padding: 2rem 2rem; }
        .header-container { max-width: 1400px; margin: 0 auto; padding: 0 0; display: flex; align-items: center; }
        .header-content { flex: 1; }
        .header-image { flex: 0 0 auto; }
        .teaser-image { max-width: 400px; width: 100%; height: auto; }
        h1 { font-size: 3rem; margin: 0; color: #2d3748; }
        h2 { font-size: 1.5rem; margin: 0.5rem 0; color: #4a5568; }
        .button-container { display: flex; gap: 1rem; margin-top: 1rem; flex-wrap: wrap; }
        .button { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.75rem 1rem; background: #3182ce; color: white; text-decoration: none; border-radius: 0.375rem; transition: background-color 0.2s; }
        .button:hover { background: #2c5aa0; }
        
        /* Basic Styles */
        pre, code { font-size: 16px; }

        /* Author link styling */
        .author-block a {
            color: #363636;
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: all 0.2s ease;
        }
        
        .author-block a:hover {
            color: var(--mgt-red);
            border-bottom-color: var(--mgt-red);
        }
        
        d-figure {
        display: block;
        width: 100%;
        text-align: center;
        margin: 20px 0;
        }

        d-figure figure {
        margin: 0 auto;
        display: inline-block;
        max-width: 100%;
        }

        d-figure img {
        display: block;
        margin: 0 auto;
        max-width: 100%;
        height: auto;
        }
        
        d-figure figcaption {
            text-align: justify;
            margin-top: 15px;
            padding: 0 20px;
            max-width: 1000px;
            margin-left: auto;
            margin-right: auto;
        }
        
        /* Leaderboard Styles */
        .leaderboard-table {
            width: 140%;
            margin-left: -20%;
            border-collapse: separate;
            border-spacing: 0;
            background: #ffffff;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            font-size: 14px;
            margin-bottom: 40px;
        }

        .leaderboard-container { position: relative; }
        
        .leaderboard-table thead {
            background: #4a5568;
            color: white;
        }

        .leaderboard-table th {
            padding: 14px 12px;
            text-align: center;
            font-weight: 600;
            font-size: 13px;
            letter-spacing: 0.025em;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            position: relative;
        }

        .sortable-header {
            cursor: pointer;
            user-select: none;
        }
        
        .sortable-header .sort-indicator {
            position: absolute;
            right: 8px;
            top: 50%;
            font-size: 14px;
            line-height: 1;
            transform: translateY(-50%);
            color: #a0aec0;
        }

        .sortable-header.active .sort-indicator { color: white; }
        
        .section-header {
            background-color: #deeef8;
            font-weight: 700;
            color: #2d3748;
            letter-spacing: 1px;
            font-size: 18px;
            text-align: center;
            padding: 12px !important;
            border-bottom: 2px solid #e2e8f0;
        }

        .leaderboard-table td {
            padding: 12px;
            border-bottom: 1px solid #e2e8f0;
            color: #4a5568;
            text-align: center;
        }
        
        .leaderboard-table tbody tr {
            transition: background-color 0.15s ease;
        }
        
        .leaderboard-table tbody tr:hover {
            background-color: #f7fafc;
        }

        .leaderboard-table td:first-child {
            font-weight: 600;
            text-align: left;
            color: #2d3748;
            padding-left: 20px;
        }
        
        .leaderboard-table td:nth-child(5) {
            font-weight: 700;
            font-size: 15px;
            color: #2d3748;
        }
        
        .human-level-row {
            background: #fffbeb85 !important;
            font-weight: 600;
        }
        
        .top-performer {
            background: #f0fdf494 !important;
        }

        .top-performer td:first-child::before {
            content: 'ü•á ';
        }

        .model-badge {
            display: inline-block;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 11px;
            font-weight: 500;
            margin-left: 8px;
            text-transform: uppercase;
            letter-spacing: 0.025em;
        }

        .proprietary-badge {
            background: #e2e8f0;
            color: #475569;
            border: 1px solid #cbd5e0;
        }

        .open-badge {
            background: #e6fffa;
            color: #047857;
            border: 1px solid #a7f3d0;
        }
        
        .leaderboard-table tbody tr:nth-child(even) {
            background-color: #fafafa;
        }
        
        .baseline-row {
            font-style: italic;
            opacity: 0.8;
            background-color: #f9fafb;
        }
        
        .best-score {
            font-weight: 700;
            text-decoration: underline;
            text-decoration-color: #cbd5e0;
            text-underline-offset: 2px;
        }

        .section-divider {
            border-top: 2px solid #e2e8f0;
        }

        .highlight-box {
            background-color: #f0f9ff;
            border-left: 4px solid #3b82f6;
            padding: 16px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .formula {
            text-align: center;
            margin: 20px 0;
            font-size: 18px;
        }

        .data-table {
            border-collapse: collapse;
            margin: 20px auto;
        }

        .data-table th,
        .data-table td {
            border: 1px solid #ddd;
            padding: 8px 12px;
            text-align: center;
        }

        .data-table thead {
            background-color: #f5f5f5;
        }
        
        /* ‰øÆÂ§çË°®Ê†ºÂÆπÂô®Â±Ö‰∏≠ */
        .table-comparison-container {
            display: flex;
            justify-content: center;
            align-items: flex-start;
            gap: 20px;
            margin: 20px 0;
        }
        
        .table-comparison-container table {
            margin: 0;
        }
        
        /* Mobile Responsive */
        @media (max-width: 768px) {
            .leaderboard-container {
                overflow-x: auto;
                -webkit-overflow-scrolling: touch;
                scroll-behavior: smooth;
                border-radius: 8px;
                box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            }
            
            .leaderboard-table {
                width: 1200px;
                margin-left: 0;
                font-size: 12px;
                min-width: 1200px;
            }
            
            .leaderboard-table th,
            .leaderboard-table td {
                padding: 8px;
                white-space: nowrap;
                min-width: 80px;
            }
            
            .leaderboard-table td:first-child {
                min-width: 180px;
                max-width: 180px;
                overflow: hidden;
                text-overflow: ellipsis;
                padding-left: 12px;
            }

            .leaderboard-container::after {
                content: "‚Üê Swipe to scroll ‚Üí";
                position: absolute;
                bottom: -25px;
                left: 50%;
                transform: translateX(-50%);
                font-size: 11px;
                color: #666;
                font-style: italic;
                pointer-events: none;
            }
            
            .table-comparison-container {
                flex-direction: column;
                align-items: center;
            }
        }
    </style>

    <!-- Non-critical CSS loaded asynchronously -->
    <link rel="stylesheet" href="./static/css/style.css" media="print" onload="this.media='all'">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" media="print" onload="this.media='all'">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" media="print" onload="this.media='all'">
    
    <!-- KaTeX for Math - loaded asynchronously -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous" media="print" onload="this.media='all'">
</head>

<body>
    <!-- Header Section -->
    <div class="header-wrapper">
        <div class="header-container" id="header-container">
            <div class="header-content">
                <h1 style="margin-top: 0px"><i>ROVER</i></h1>
                
                <h2>Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</h2>

                <div>
                    <!-- Paper authors -->
                    <span class="author-block">
                        <a href="" target="_blank">
                            Yongyuan Liang</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,</span>
                    <span class="author-block">
                        <a href="" target="_blank">
                            Wei Chow</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,</span>
                    <span class="author-block">
                        <a href="" target="_blank">Feng Li</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,</span>
                    <span class="author-block">
                        <a href="" target="_blank">Ziqiao Ma</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,
                    </span>
                    <span class="author-block">
                        <a href="" target="_blank">Xiyao Wang</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,
                    </span>
                    <span class="author-block">
                        <a href="" target="_blank">Jiageng Mao</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,
                    </span>
                    <br>
                    <span class="author-block">
                        <a href="" target="_blank">Jiuhai Chen</a><sup style="font-size: 0.8em;">‚ô£</sup>,
                    </span>
                    <span class="author-block">
                        <a href="" target="_blank">Jiatao Gu</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,
                    </span>
                    <span class="author-block">
                        <a href="" target="_blank">Yue Wang</a><sup style="font-size: 0.8em;">‚òÖ</sup>,
                    </span>
                    <span class="author-block">
                        <a href="" target="_blank">Furong Huan</a><sup style="font-size: 0.8em;">‚ñ≥</sup>,
                    </span>
                </div>

                <div>
                    <span class="author-block"><sup style="font-size: 0.8em;">‚ñ≥</sup>University of Maryland, College Park,</span>
                    <span class="author-block"><sup style="font-size: 0.8em;">‚ñ≤</sup>University of Pennsylvania,</span>
                    <span class="author-block"><sup style="font-size: 0.8em;">‚òÖ</sup>University of Southern California,</span>
                    <br>
                    <span class="author-block"><sup style="font-size: 0.8em;">‚ô£</sup>University of Michigan</span>
                    <span class="author-block"><sup style="font-size: 0.8em;">‚ô¶</sup>The Hong Kong University of Science and
                        Technology</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution.</small></span>
                </div>

                <div class="button-container">
                    <a href="https://arxiv.org/abs/2412.14171" class="button paper-link" target="_blank">
                        <span class="icon is-small"><i class="ai ai-arxiv"></i></span>
                        arXiv (soon)
                    </a>
                    <a href="https://github.com/vision-x-nyu/thinking-in-space" class="button" target="_blank">
                        <span class="icon is-small"><i class="fab fa-github"></i></span>
                        <span>Code (soon)</span>
                    </a>                      
                    <a href="https://huggingface.co/datasets/nyu-visionx/VSI-Bench" class="button" target="_blank">
                        <span class="icon is-small">
                            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;" loading="lazy">
                        </span>
                        <span>ROVER (soon)</span>
                    </a>
                    <a href="#vsi-leaderboard" class="button">
                        <span class="icon is-small">
                            <img src="./static/img/leaderboard-star-svgrepo-com.svg" alt="Leaderboard logo" style="height: 1em;" loading="lazy">
                        </span>
                        <span>Leaderboard</span>
                    </a>
                </div>   

            </div>
        </div>
    </div>

    <d-article>
        <!-- Teaser Figure -->
        <d-figure id="fig-teaser">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Visual-Spatial Intelligence Teaser" loading="lazy">
                <figcaption>
                    <strong>Figure 1:</strong> The <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> benchmark. <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> evaluates UMMs through reciprocal cross-modal reasoning: \ourvg (left) requires generating images with language-augmented reasoning, while \ourir (right) requires generating text answers with visually-augmented reasoning.
                </figcaption>
            </figure>
        </d-figure>

        <!-- Abstract -->
        <p class="text">
            Unified multimodal models (UMMs) have shown remarkable advances in understanding and generating text and images. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning. Existing benchmarks rarely require the use of one modality to guide, verify, or refine outputs in the other, failing to capture a central aspiration of unified multimodal models: seamless reasoning across modalities. We address this gap with <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span>, a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, containing over 1,200 tasks grounded in 2,048 images spanning two complementary settings.
        </p>

        <div class="icon-container">
            <div class="icon-item">
                <img src="./static/img/icons/bench.svg" alt="Benchmark Icon" loading="lazy">
                <div><strong>Reciprocal Reasoning</strong>: We introduce the first benchmark targeting reciprocal cross-modal reasoning where one modality guides, verifies, or refines outputs in another.</div>
            </div>
            <div class="icon-item">
                <img src="./static/img/icons/evaluation.svg" alt="Evaluation Icon" loading="lazy">
                <div><strong>Dual Settings</strong>: <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> evaluates verbally-augmented visual generation and visually-augmented verbal reasoning across diverse domains and reasoning types.</div>
            </div>
            <div class="icon-item">
                <img src="./static/img/icons/speech.svg" alt="Generation Icon" class="icon" loading="lazy">
                <div><strong>Comprehensive Evaluation</strong>: Multi-dimensional evaluation protocol assessing reasoning process, output alignment, and cross-modal consistency.</div>
            </div>
            <div class="icon-item">
                <img src="./static/img/icons/vision.svg" alt="Analysis Icon" class="icon" loading="lazy">
                <div><strong>Key Insights</strong>: Cross-modal reasoning strongly correlates with visual generation performance, while current models show limited visually-augmented reasoning capabilities.</div>
            </div>
        </div>

        <!-- Navigation Icons -->
        <div class="icon-row">
            <a href="#rover-benchmark" class="icon-link">
                <img src="./static/img/icons/bench.svg" alt="Benchmark Logo" class="icon" loading="lazy">
                ROVER<br>Benchmark
            </a>
            <a href="#evaluation-protocol" class="icon-link">
                <img src="./static/img/icons/evaluation.svg" alt="Evaluation Logo" class="icon" loading="lazy">
                Evaluation<br>Protocol
            </a>
            <a href="#rover-leaderboard" class="icon-link">
                <img src="./static/img/leaderboard-star-svgrepo-com.svg" alt="Leaderboard Logo" class="icon" loading="lazy">
                ROVER<br>Leaderboard
            </a>
            <a href="#experimental-results" class="icon-link">
                <img src="./static/img/icons/speech.svg" alt="Results Logo" class="icon" loading="lazy">
                Experimental<br>Results
            </a>
            <a href="#analysis-insights" class="icon-link">
                <img src="./static/img/icons/vision.svg" alt="Analysis Logo" class="icon" loading="lazy">
                Analysis &<br>Insights
            </a>
        </div>

        <p class="click-hint2" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem" loading="lazy">
            <strong>Click to jump to each section.</strong>
        </p>

        <hr>

        <!-- ROVER Benchmark Section -->
        <div id='rover-benchmark' class="rover-benchmark">
            <div id="sec:rover-overview" class="sub-section">
                <h1 class="text"><span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> Benchmark</h1>
                <p class="text" align="justify">
                    <strong>Benchmark Overview:</strong> <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> introduces the first benchmark specifically designed to evaluate reciprocal cross-modal reasoning in unified multimodal models. Unlike existing benchmarks that evaluate modalities in isolation, <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> requires models to use information from one modality to inform and improve outputs in another. The benchmark comprises over 1,200 tasks grounded in about 2,048 images, targeting two complementary settings that capture the essence of cross-modal reasoning capabilities.
                </p>
                
                <d-figure id="fig-verbally-augmented">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/data.png" alt="Verbally-Augmented Reasoning" loading="lazy">
                        <figcaption style="text-align: center; margin-top: 20px;">
                            <strong>Figure 2: Verbally-Augmented Reasoning for Visual Generation.</strong> 
                            The benchmark spans 4 domains (natural science, culture and art, common sense, and logic), instantiated across 7 reasoning subtasks.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <d-figure id="fig-visually-augmented">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/data_gtr.png" alt="Visually-Augmented Reasoning" loading="lazy">
                        <figcaption style="text-align: center; margin-top: 20px;">
                            <strong>Figure 3: Visually-Augmented Reasoning for Verbal Generation.</strong> 
                            The benchmark spans 3 scenarios and 6 subtasks: physical world modeling, logical assistance, and visual perception enhancement.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
            
            <div id="benchmark-construction" class="sub-section">
                <p class="text" align="justify">
                    <strong>Verbally-Augmented Reasoning for Visual Generation:</strong>
                    This setting evaluates whether models can use structured verbal prompts and reasoning chains to guide faithful image synthesis. It spans 4 domains (natural science, culture and art, common sense, and logic) instantiated across 7 reasoning types: temporal, spatial, causal, synthetic, quantitative, abstract, and mathematical. Each task provides a textual prompt with an initial image and a chain of constraints that a correct output image must satisfy, requiring genuine visual understanding and complex reasoning chains.
                </p>

                <p class="text" align="justify">
                    <strong>Visually-Augmented Reasoning for Verbal Generation:</strong>
                    This setting evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes. Unlike text-only Chain-of-Thought, we examine scenarios where models generate intermediate visual representations to facilitate reasoning. The benchmark focuses on 3 scenarios: physical world modeling (functioning as world simulators), logical assistance (generating visual aids for abstract problems), and visual perception enhancement (creating supportive images for challenging perception tasks).
                </p>

                <d-figure id="fig-example-outputs">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/example1.png" alt="Example Outputs VG" loading="lazy">
                        <figcaption>
                            <p align="justify"><strong>Figure 4: Example outputs on verbally-augmented reasoning.</strong> Each row corresponds to one reasoning subtask, with the input on the left and outputs from representative unified multimodal models shown across columns.</p>
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
        </div>

        <!-- Evaluation Protocol Section -->
        <div id="evaluation-protocol" class="evaluation-protocol">
            <h1 class="text">Evaluation Protocol</h1>
            
            <p class="text" align="justify">
                <strong>Multi-Dimensional Assessment</strong>:
                Evaluating reciprocal cross-modal reasoning requires assessment of both reasoning steps and resulting outputs. Text-only metrics overlook visual fidelity, while image-only metrics cannot verify whether the image reflects valid reasoning. We adopt a multi-dimensional protocol that combines an automated VLM judge with expert validation on stratified samples.
            </p>
        
            <p class="text" align="justify">
                <strong>Verbally-Augmented Generation Metrics</strong>:
                We assess model performance across 5 rubric dimensions: (1) <strong>Reasoning Process (RP)</strong> evaluates the quality of verbal reasoning through logical structure and domain knowledge application; (2) <strong>Reasoning Visual (RV)</strong> measures how well generated visuals match target descriptions; (3) <strong>Reasoning Alignment (Align.)</strong> quantifies consistency between verbal reasoning and visual outcomes; (4) <strong>Visual Consistency (VC)</strong> ensures non-target elements remain unchanged; (5) <strong>Image Quality (IQ)</strong> assesses technical excellence and visual coherence.
            </p>
            
            <p class="text" align="justify">
                <strong>Visually-Augmented Generation Metrics</strong>:
                We evaluate across 3 dimensions: (1) <strong>Interleaved Reasoning Quality (IR)</strong> evaluates plausibility and relevance of intermediate visual representations; (2) <strong>Final Answer Accuracy (Acc.)</strong> measures whether the model's final reasoning outcome matches ground truth; (3) <strong>Reasoning-Answer Alignment (Align.)</strong> quantifies how effectively generated images contribute to reaching correct conclusions.
            </p>
            
            <d-figure id="fig-example-interleaved">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/example2.png" alt="Example Interleaved Outputs" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Figure 5: Example outputs on visually-augmented reasoning.</strong> Each row corresponds to one reasoning scenario, with the input on the left and outputs from representative unified models shown across columns.
                    </figcaption>
                </figure>
            </d-figure>
        </div>

<!-- Evaluation Protocol Section -->
        <div id="evaluation-protocol" class="evaluation-protocol">
            <h1 class="text">Evaluation Protocol</h1>
            
            <p class="text" align="justify">
                <strong>Multi-Dimensional Assessment</strong>:
                Evaluating reciprocal cross-modal reasoning requires assessment of both reasoning steps and resulting outputs. Text-only metrics overlook visual fidelity, while image-only metrics cannot verify whether the image reflects valid reasoning. We adopt a multi-dimensional protocol that combines an automated VLM judge with expert validation on stratified samples.
            </p>
        
            <p class="text" align="justify">
                <strong>Verbally-Augmented Generation Metrics</strong>:
                We assess model performance across 5 rubric dimensions: (1) <strong>Reasoning Process (RP)</strong> evaluates the quality of verbal reasoning through logical structure and domain knowledge application; (2) <strong>Reasoning Visual (RV)</strong> measures how well generated visuals match target descriptions; (3) <strong>Reasoning Alignment (Align.)</strong> quantifies consistency between verbal reasoning and visual outcomes; (4) <strong>Visual Consistency (VC)</strong> ensures non-target elements remain unchanged; (5) <strong>Image Quality (IQ)</strong> assesses technical excellence and visual coherence.
            </p>
            
            <p class="text" align="justify">
                <strong>Visually-Augmented Generation Metrics</strong>:
                We evaluate across 3 dimensions: (1) <strong>Interleaved Reasoning Quality (IR)</strong> evaluates plausibility and relevance of intermediate visual representations; (2) <strong>Final Answer Accuracy (Acc.)</strong> measures whether the model's final reasoning outcome matches ground truth; (3) <strong>Reasoning-Answer Alignment (Align.)</strong> quantifies how effectively generated images contribute to reaching correct conclusions.
            </p>
            
            <d-figure id="fig-example-interleaved">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/example2.png" alt="Example Interleaved Outputs" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Figure 5: Example outputs on visually-augmented reasoning.</strong> Each row corresponds to one reasoning scenario, with the input on the left and outputs from representative unified models shown across columns.
                    </figcaption>
                </figure>
            </d-figure>
        </div>

        <!-- ROVER Leaderboard Section -->
        <div id="rover-leaderboard" class="rover-leaderboard">
            <h1 class="text"><span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> Leaderboard</h1>
            
            <div class="leaderboard-container">
                <div class="leaderboard-header">
                    <p class="text" align="justify">
                        To include your model in the leaderboard, please email <a href="mailto:jihanyang13@gmail.com">jihanyang13@gmail.com</a> with evaluation logs and setups.
                    </p>
                </div>

                <table class="leaderboard-table">
                    <thead>
                        <tr class="header-group">
                            <th rowspan="2" data-column-index="0">Model</th>
                            <th rowspan="2" data-column-index="1">LLM Params</th>
                            <th rowspan="2" data-column-index="2">Frames</th>
                            <th rowspan="2" data-column-index="3">Date</th>
                            <th rowspan="2" data-column-index="4">Avg.</th>
                            <th colspan="4">Numerical Answer Questions</th>
                            <th colspan="4">Multiple-Choice Questions</th>
                        </tr>
                        <tr>
                            <th data-column-index="5">Obj. Count</th>
                            <th data-column-index="6">Abs. Dist.</th>
                            <th data-column-index="7">Obj. Size</th>
                            <th data-column-index="8">Room Size</th>
                            <th data-column-index="9">Rel. Dist.</th>
                            <th data-column-index="10">Rel. Dir.</th>
                            <th data-column-index="11">Route Plan</th>
                            <th data-column-index="12">Appear. Order</th>
                        </tr>
                    </thead>
                    <tbody id="leaderboard-body">
                        <tr class="section-header">
                            <td colspan="13">Baselines</td>
                        </tr>
                        <tr class="baseline-row">
                            <td>Chance-level (Random)</td>
                            <td>-</td>
                            <td>-</td>
                            <td>2024-11-15</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>25.0</td>
                            <td>36.1</td>
                            <td>28.3</td>
                            <td>25.0</td>
                        </tr>
                        <tr class="baseline-row">
                            <td>Chance-level (Frequency)</td>
                            <td>-</td>
                            <td>-</td>
                            <td>2024-11-15</td>
                            <td>34.0</td>
                            <td>62.1</td>
                            <td>32.0</td>
                            <td>29.9</td>
                            <td>33.1</td>
                            <td>25.1</td>
                            <td>47.9</td>
                            <td>28.4</td>
                            <td>25.2</td>
                        </tr>
                        <tr class="human-level-row">
                            <td><strong>Human-Level</strong></td>
                            <td>-</td>
                            <td>-</td>
                            <td>2024-11-15</td>
                            <td class="best-score">79.2</td>
                            <td class="best-score">94.3</td>
                            <td>47.0</td>
                            <td>60.4</td>
                            <td>45.9</td>
                            <td class="best-score">94.7</td>
                            <td class="best-score">95.8</td>
                            <td class="best-score">95.8</td>
                            <td class="best-score">100.0</td>
                        </tr>
                        <tr class="section-divider">
                            <td colspan="13" class="section-header">MLLMs</td>
                        </tr>
                        <tr>
                            <td>Gemini-1.5-Pro <span class="model-badge proprietary-badge">Proprietary</span></td>
                            <td>-</td>
                            <td>-</td>
                            <td>2024-11-15</td>
                            <td>48.8</td>
                            <td>49.6</td>
                            <td>28.8</td>
                            <td>58.6</td>
                            <td>49.4</td>
                            <td>46.0</td>
                            <td>48.1</td>
                            <td>42.0</td>
                            <td>68.0</td>
                        </tr>
                        <tr>
                            <td>InternVL3-78B <span class="model-badge open-badge">Open</span></td>
                            <td>78B</td>
                            <td>64</td>
                            <td>2025-04-19</td>
                            <td>48.4</td>
                            <td>71.2</td>
                            <td>53.7</td>
                            <td>44.4</td>
                            <td>39.5</td>
                            <td>55.9</td>
                            <td>39.5</td>
                            <td>28.9</td>
                            <td>54.5</td>
                        </tr>
                        <tr>
                            <td>GPT-4o <span class="model-badge proprietary-badge">Proprietary</span></td>
                            <td>-</td>
                            <td>64</td>
                            <td>2025-04-03</td>
                            <td>47.8</td>
                            <td>43.1</td>
                            <td>34.1</td>
                            <td>68.6</td>
                            <td>64.2</td>
                            <td>48.3</td>
                            <td>43.1</td>
                            <td>29.4</td>
                            <td>51.3</td>
                        </tr>
                        <tr>
                            <td>Gemini-1.5 Flash <span class="model-badge proprietary-badge">Proprietary</span></td>
                            <td>-</td>
                            <td>-</td>
                            <td>2024-11-15</td>
                            <td>45.7</td>
                            <td>50.8</td>
                            <td>33.6</td>
                            <td>56.5</td>
                            <td>45.2</td>
                            <td>48.0</td>
                            <td>39.8</td>
                            <td>32.7</td>
                            <td>59.2</td>
                        </tr>
                        <tr>
                            <td>Gemini-2.0 Flash <span class="model-badge proprietary-badge">Proprietary</span></td>
                            <td>-</td>
                            <td>-</td>
                            <td>2024-12-20</td>
                            <td>45.4</td>
                            <td>52.4</td>
                            <td>30.6</td>
                            <td>66.7</td>
                            <td>31.8</td>
                            <td>56.0</td>
                            <td>46.3</td>
                            <td>24.5</td>
                            <td>55.1</td>
                        </tr>
                        <tr>
                            <td>LLaVA-Video-72B <span class="model-badge open-badge">Open</span></td>
                            <td>72B</td>
                            <td>32</td>
                            <td>2024-11-24</td>
                            <td>40.9</td>
                            <td>48.9</td>
                            <td>22.8</td>
                            <td>57.4</td>
                            <td>35.3</td>
                            <td>42.4</td>
                            <td>36.7</td>
                            <td>35.0</td>
                            <td>48.6</td>
                        </tr>
                        <tr>
                            <td>LLaVA-OneVision-72B <span class="model-badge open-badge">Open</span></td>
                            <td>72B</td>
                            <td>32</td>
                            <td>2024-11-27</td>
                            <td>40.2</td>
                            <td>43.5</td>
                            <td>23.9</td>
                            <td>57.6</td>
                            <td>37.5</td>
                            <td>42.5</td>
                            <td>39.9</td>
                            <td>32.5</td>
                            <td>44.6</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <!-- Experimental Results Section -->
        <div id='experimental-results' class="experimental-results">
            <h1 class="text">Experimental Results</h1>
                
            <p class="text" align="justify">
                We conducted comprehensive evaluation of 17 state-of-the-art unified multimodal models across both settings in <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span>. Our experiments reveal critical insights about the current state and limitations of cross-modal reasoning capabilities in modern UMMs.
            </p>

            <p class="text" align="justify">
                <strong>Verbally-Augmented Visual Generation Results</strong>:
                Cross-modal reasoning capabilities and alignment strongly correlate with visual generation effectiveness. Closed-source models excel in reasoning processes and demonstrate strong alignment performance, directly contributing to superior visual generation quality. Open-source models show notably weaker verbal reasoning during visual generation tasks‚Äîtheir reasoning processes are approximately 38% lower and alignment performance falls about 31% short of closed-source models, translating into correspondingly diminished visual generation performance.
            </p>

            <d-figure id="fig-evaluation-results">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/eval1.png" alt="Evaluation Results" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Table 1</strong>: Evaluation results on verbally-augmented reasoning for visual generation across different unified multimodal models.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text" align="justify">
                <strong>Visually-Augmented Verbal Generation Results</strong>:
                Current unified models exhibit limited capacity in interleaved reasoning, constraining their ability to leverage cross-modal reasoning for improved performance. Even the best-performing models struggle with interleaved reasoning processes, with the highest average Interleaved Reasoning (IR) score reaching only 39.5% overall. Models demonstrate superior performance on physical world modeling and visual perception tasks compared to logical reasoning challenges, indicating that perception over pixels transfers more readily than acquisition of abstract visual concepts.
            </p>
            
            <d-figure id="fig-interleaved-results">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/eval2.png" alt="Interleaved Results" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Table 2</strong>: Evaluation results on visually-augmented reasoning for verbal generation showing limited interleaved reasoning capabilities.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="highlight-box">
                <em>Key Finding 1: Cross-modal reasoning capabilities strongly correlate with visual generation performance, particularly for interleaved image-text generation.</em>
            </div>

            <div class="highlight-box">
                <em>Key Finding 2: Current models remain severely limited in visually-augmented reasoning, showing relative strength in perception and physical modeling but weakness in logical tasks.</em>
            </div>
        </div>

        <!-- Analysis and Insights Section -->
        <div id='analysis-insights' class="analysis-insights">
            <h1 class="text">Analysis and Insights</h1>
            
            <p class="text" align="justify">
                <strong>Cross-Modal Reasoning Matters for UMMs</strong>:
                To validate that UMMs perform cross-modal reasoning internally and that this mechanism cannot be replicated through external models, we conducted comparative analysis between unified models and cascade approaches. Results demonstrate that reasoning across modalities cannot fully transfer across different model architectures‚Äîunified models must transcend modality boundaries to produce emergent cross-modal insights.
            </p>

            <d-figure id="fig-cascade-analysis">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/cascade.png" alt="Cascade Analysis" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Figure 6</strong>: Cascade reasoning evaluation comparing cascade approaches (FLUX+GPT with GPT-4o prompt refinement) against unified multimodal models.
                    </figcaption>
                </figure>
            </d-figure>
            
            <p class="text" align="justify">
                <strong>Coherence Between Reasoning Subtasks</strong>:
                Analysis reveals uneven performance across reasoning dimensions, with models excelling in temporal, spatial, and causal reasoning while struggling with abstract and mathematical tasks. This pattern indicates that current UMMs better handle concrete, observable phenomena than symbolic reasoning. Strong interdependence among physical reasoning types suggests shared mechanisms for processing spatiotemporal relationships, while abstract reasoning develops as a distinct capability.
            </p>
            
            <d-figure id="fig-reasoning-analysis">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/reasoning.png" alt="Reasoning Analysis" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Figure 7</strong>: Analysis of reasoning capabilities showing performance patterns across different reasoning subtasks and their correlations.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text" align="justify">
                <strong>Evaluation Protocol Reliability</strong>:
                We conducted user studies with 4 human experts to validate our VLM-as-judge evaluation protocol. Results demonstrate strong alignment between GPT-4.1 and human expert judgments across all evaluation dimensions. Visual-quality-related metrics show particularly strong human-VLM agreement, while reasoning-related metrics exhibit larger but acceptable discrepancies due to inherent complexities in multimodal reasoning assessment.
            </p>
            
            <d-figure id="fig-evaluation-reliability">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/vlm_evaluation_metrics.png" alt="Evaluation Reliability" loading="lazy">
                    <figcaption style="text-align: center; margin-top: 20px;">
                        <strong>Figure 8</strong>: Evaluation reliability of GPT-4.1 across five assessment dimensions, showing Pearson correlation coefficients and Mean Absolute Error compared to human experts.
                    </figcaption>
                </figure>
            </d-figure>
        </div>

        <!-- Conclusion Section -->
        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text" align="justify">
                We introduce <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span>, the first benchmark for reciprocal cross-modal reasoning, which systematically evaluates 17 unified multimodal models across 23 diverse task types in both verbal reasoning for visual generation and interleaved multimodal reasoning scenarios. Our evaluation exposes substantial performance gaps in current models and establishes that interleaved generation capabilities are strongly correlated with cross-modal reasoning effectiveness. These findings expose critical limitations in existing unified models and provide insights for advancing cross-modal reasoning capabilities in future omnimodal models. <span class="rover-logo">
  <span class="r1">R</span><span class="o">O</span><span class="v">V</span><span class="e">E</span><span class="r2">R</span>
</span> represents a critical step toward enabling true omnimodal generation through reciprocal cross-modal reasoning.
            </p>
        </div>

        <!-- BibTeX Section -->
        <div id="bibtex" style="position: relative; margin-top: 40px; margin-bottom: 0px; color: gray;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">BibTeX</h2>
            <pre><code>@article{yang2024think,
    title={{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},
    author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali W. and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
    year={2024},
    journal={arXiv preprint arXiv:2412.14171},
}</code></pre>
        </div>
    </d-article>

    <!-- Non-critical scripts loaded asynchronously -->
    <script>
        // Load scripts asynchronously after page load
        function loadScript(src, callback) {
            const script = document.createElement('script');
            script.src = src;
            script.async = true;
            if (callback) script.onload = callback;
            document.head.appendChild(script);
        }

        window.addEventListener('load', function() {
            // Load non-critical scripts
            loadScript('./static/js/distill_template.v2.js');
            loadScript('https://polyfill.io/v3/polyfill.min.js?features=es6');
            loadScript('https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js');
            loadScript('https://d3js.org/d3.v5.min.js');
            loadScript('https://d3js.org/d3-collection.v1.min.js');
            loadScript('https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js');
            loadScript('./static/js/hider.js');
            loadScript('./static/js/image_interact.js');
            loadScript('./static/js/switch_videos.js');
            loadScript('./static/js/video-speed.js');
            loadScript('./static/js/fontawesome.all.min.js');
            loadScript('https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js');
            loadScript('./static/js/medium-zoom.min.js');
            loadScript('./static/js/zoom.js');
            loadScript('https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js', function() {
                loadScript('https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js', function() {
                    renderMathInElement(document.body);
                });
            });
        });
    </script>

    <!-- Leaderboard Sorting JavaScript -->
    <script>
    document.addEventListener('DOMContentLoaded', function () {
        const table = document.querySelector('.leaderboard-table');
        if (!table) return;
        
        const thead = table.querySelector('thead');
        const tbody = document.getElementById('leaderboard-body');
        
        let sortConfig = {
            columnIndex: 4, 
            direction: 'desc'
        };

        const headers = thead.querySelectorAll('th[data-column-index]');

        function updateSortIndicators() {
            headers.forEach(h => {
                const indicator = h.querySelector('.sort-indicator');
                if (indicator) {
                    const hIndex = parseInt(h.dataset.columnIndex);
                    if (hIndex === sortConfig.columnIndex) {
                        indicator.textContent = sortConfig.direction === 'asc' ? '‚ñ≤' : '‚ñº';
                        h.classList.add('active');
                    } else {
                        indicator.textContent = '';
                        h.classList.remove('active');
                    }
                }
            });
        }
        
        function parseParams(param) {
            if (param === '-') return -1;
            const value = parseFloat(param);
            if (isNaN(value)) return -1;
            if (param.toLowerCase().includes('b')) return value * 1e9;
            if (param.toLowerCase().includes('m')) return value * 1e6;
            return value;
        }

        function compareValues(valA, valB, index) {
            if (index === 0) { 
                return valA.localeCompare(valB);
            }
            if (index === 1) { 
                return parseParams(valA) - parseParams(valB);
            }
            if (index === 3) { 
                return new Date(valA) - new Date(valB);
            }

            const numA = parseFloat(valA);
            const numB = parseFloat(valB);
            const isNumA = !isNaN(numA);
            const isNumB = !isNaN(numB);

            if (isNumA && isNumB) return numA - numB;
            if (isNumA) return 1;
            if (isNumB) return -1;
            return 0;
        }

        function sortTable() {
            const rows = Array.from(tbody.querySelectorAll('tr'));
            
            const sortableRows = rows.filter(row => 
                !row.classList.contains('baseline-row') &&
                !row.classList.contains('human-level-row') &&
                !row.classList.contains('section-divider') &&
                !row.classList.contains('section-header')
            );
            
            const direction = sortConfig.direction === 'asc' ? 1 : -1;

            sortableRows.sort((rowA, rowB) => {
                const cellA = rowA.children[sortConfig.columnIndex].textContent.trim();
                const cellB = rowB.children[sortConfig.columnIndex].textContent.trim();
                return compareValues(cellA, cellB, sortConfig.columnIndex) * direction;
            });

            sortableRows.forEach(row => tbody.appendChild(row));
            updateSortIndicators();
        }

        headers.forEach(header => {
            header.classList.add('sortable-header');
            const indicator = document.createElement('span');
            indicator.className = 'sort-indicator';
            header.appendChild(indicator);

            header.addEventListener('click', () => {
                const columnIndex = parseInt(header.dataset.columnIndex);
                if (sortConfig.columnIndex === columnIndex) {
                    sortConfig.direction = sortConfig.direction === 'asc' ? 'desc' : 'asc';
                } else {
                    sortConfig.columnIndex = columnIndex;
                    sortConfig.direction = 'desc';
                }
                sortTable();
            });
        });

        sortTable();
    });
    </script>
</body>
</html>